{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apache Spark 개요 및 어플리케이션 구조\n",
    "\n",
    "#### 1. Apache Spark 소개\n",
    "- 개요: Apache Spark는 2010년 UC Berkeley's AMPLab에서 개발된 오픈 소스 클러스터 컴퓨팅 프레임워크로, 대규모 데이터 처리에 최적화되어 있습니다. 메모리 기반의 데이터 처리 방식을 통해 성능을 극대화합니다.\n",
    "\n",
    "#### 2. 등장 배경\n",
    "- 대량의 데이터 처리 필요성: 데이터 양의 급증으로 인해 효율적인 데이터 처리 도구의 필요성이 대두되었습니다.\n",
    "- Hadoop의 한계: 기존의 Hadoop MapReduce는 디스크 기반 I/O 작업이 많아 성능이 제한적이며, 반복적인 데이터 처리에 비효율적입니다.\n",
    "- 메모리 기반 처리 필요성: 대규모 데이터를 빠르게 처리할 수 있는 메모리 기반 시스템의 필요성이 증가했습니다.\n",
    "\n",
    "#### 3. Spark의 특징\n",
    "- 빠른 처리 속도: 메모리에서 데이터를 처리하여 기존 Hadoop보다 10배에서 100배 더 빠른 성능을 제공합니다.\n",
    "- 다양한 API: Java, Scala, Python, R 등 다양한 언어를 지원하며, SQL 쿼리, 데이터프레임, 머신러닝, 그래프 처리 등 여러 라이브러리를 제공합니다.\n",
    "- 유연한 처리 모델: 배치 처리, 실시간 스트리밍, 머신러닝 및 그래프 처리 등 다양한 데이터 처리 작업을 지원합니다.\n",
    "- 내결함성: RDD(Resilient Distributed Dataset)를 통해 데이터 손실에 대한 내결함성을 보장합니다.\n",
    "\n",
    "#### 4. SparkSession\n",
    "- 정의: SparkSession은 Spark 애플리케이션의 진입점으로, Spark의 다양한 기능을 사용할 수 있게 해주는 클래스입니다.\n",
    "- 사용법: SparkSession을 통해 다양한 API에 접근할 수 있습니다.\n",
    "- 예시:\n",
    "  ```python\n",
    "  from pyspark.sql import SparkSession\n",
    "\n",
    "  # SparkSession 생성\n",
    "  spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "  \n",
    "#### 5. RDD (Resilient Distributed Dataset)\n",
    "- 정의: RDD는 Spark의 핵심 데이터 구조로, 대규모 데이터를 분산된 환경에서 처리할 수 있게 해주는 불변(immutable) 데이터 컬렉션입니다.\n",
    "- 강점: 유연성, 저수준 API, 내결함성 등으로 복잡한 데이터 처리 작업을 지원합니다.\n",
    "\n",
    "#### 6. 데이터프레임\n",
    "- 정의: 데이터프레임은 구조화된 데이터로, RDD에 비해 성능과 편의성을 개선한 데이터 구조입니다. 스키마를 가진 테이블 형식으로 구성됩니다.\n",
    "- 비교:\n",
    "  - 편의성: 데이터프레임은 SQL 쿼리와 유사한 방식으로 데이터를 다룰 수 있어 사용이 간편합니다.\n",
    "  - 최적화: Spark SQL의 Catalyst Optimizer를 통해 데이터프레임 연산은 더욱 최적화되어 성능이 향상됩니다.\n",
    "  - 구조화된 데이터: 데이터프레임은 RDD에 비해 명확한 스키마를 가지고 있어 데이터의 구조를 이해하고 처리하는 데 도움이 됩니다.\n",
    "\n",
    "#### 7. 잡(Job)\n",
    "- 정의: 잡은 Spark 애플리케이션의 실행 단위로, 사용자가 Spark에 제출하는 연산의 집합을 나타냅니다. 여러 스테이지로 구성됩니다.\n",
    "\n",
    "#### 8. 스테이지(Stage)\n",
    "- 정의: 스테이지는 잡 내에서 데이터 처리의 특정 단계로, RDD의 분할 및 변환을 통해 생성됩니다. DAG(Directed Acyclic Graph)로 표현됩니다.\n",
    "- 특징: 샤플링, 파라렐 처리, 에러 처리 등이 가능합니다.\n",
    "\n",
    "#### 9. 스키마와 데이터 타입\n",
    "- 스키마: 데이터프레임의 구조를 정의하는 메타데이터로, 각 열의 이름과 데이터 타입을 포함합니다. 스키마를 정의하면 데이터프레임을 생성할 때 데이터를 올바르게 해석할 수 있습니다.\n",
    "- 데이터 타입: Spark에서 사용되는 데이터 타입에는 다음과 같은 기본 타입이 포함됩니다:\n",
    "  - StringType: 문자열 데이터\n",
    "  - IntegerType: 정수 데이터\n",
    "  - FloatType: 부동 소수점 데이터\n",
    "  - DoubleType: 더블 데이터\n",
    "  - BooleanType: 불리언 데이터\n",
    "  - TimestampType: 날짜 및 시간 데이터\n",
    "  - ArrayType: 배열 데이터\n",
    "  - MapType: 키-값 쌍으로 구성된 데이터\n",
    "\n",
    "- 스키마 정의 예시:\n",
    "  ```python\n",
    "  from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "  # 스키마 정의\n",
    "  schema = StructType([\n",
    "      StructField(\"Name\", StringType(), True),\n",
    "      StructField(\"Age\", IntegerType(), True)\n",
    "  ])\n",
    "\n",
    "  # 데이터프레임 생성 시 스키마 사용\n",
    "  df = spark.createDataFrame(data, schema=schema)\n",
    "  df.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
